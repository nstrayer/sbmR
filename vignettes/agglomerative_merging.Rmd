---
title: "Agglomerative Merging"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{agglomerative_merging}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6
)

library(sbmR)
library(tidyverse)
```

One technique of finding the optimal number of clusters/ best partitioning is using the efficient agglomerative merging algorithm. This is typically used to find the initial state for MCMC chains but can stand on its own as a clustering method. 

Agglomerative merging can be acomplished with the SBM class using the functions `collapse_blocks()` and `collapse_runs()`. `collapse_blocks()` runs a single agglomerative merge step from one group per node all the way down to your desired number of groups and `collapse_runs()` repeats this process for a range of groups.  

Before we look at using both we need to setup some data to cluster. 

## Setup

First we load the `sbmR` package along with the `tidyverse` package. 

```{r, eval = FALSE}
library(sbmR)
library(tidyverse)
```

### Data

The data we are using will come from the included simulation function, `sim_basic_block_network()`.


```{r simulate_data}
set.seed(42) # seed for reproducability

n_blocks <- 3    # Four total groups
group_size <- 40 # W/ 50 nodes in each

network <- sim_basic_block_network(
  n_blocks = n_blocks,     
  n_nodes_per_block = group_size,  
  return_edge_propensities = TRUE
)
```

We can investigate the simulated network's true block structure by visualizing the edge propensities between the groups.

```{r visualize_propensities}
network$edge_propensities %>% 
  ggplot(aes(x = block_1, y = block_2)) +
  geom_tile(aes(fill = propensity))
```

We can also visualise the simulated data directly using the `visualize_network()` function.

```{r visualize_network}
visualize_network(network)
```

### Model

Now that we have our network we can initialize an SBM model with it. 

```{r setup_model}
# Setup SBM model
my_sbm <- create_sbm(network)
my_sbm %>% get_state() %>% head()
```


## Single merge run

First we will demonstrate agglomerative merging using the single merge step function: `collapse_blocks()`. 

There are a few important input parameters we need to consider: 

- `sigma`: Controls the rate of collapse. At each step of the collapsing the model will try and remove `current_num_nodes(1 - 1/sigma)` nodes from the model. So a larger sigma means a faster collapse rate. 
- `desired_num_blocks`: How many groups should this given merge drop down to. If the network has more than one node type this number is multiplied by the total number of types. 
- `report_all_steps`: Should the model state be provided for every merge step or just the final one? If collapsing is being used to infer hierarcichal structure in data or inspection is desired this should be set to `TRUE`, otherwise it will slow down collapsing due to increased data transfer. 
- `greedy`: Should every possible group merger be considered? If `FALSE`, candidates for mergers are drawn by similarity in edges (just as MCMC move proposals are). This may lead the model to local minimums by always pursuing best possible merges.
- `num_block_proposals`: If `greedy = FALSE`, this parameter controls how many merger proposals are drawn for each group in the model. A larger number will increase the exploration of merge potentials but may lead the model to local minimums for the same reason greedy mode does. 
- `num_mcmc_sweeps`: How many MCMC sweeps the model does at each agglomerative merge step. This allows the model to allow nodes to find their most natural resting place in a given collapsed state. Larger values will slow down runtime but can potentially lead for more stable results. 
- `beta`: Inverse temperature parameter impacting the acceptance probabilities of MCMC sweeps. A higher value means model puts more weight on accepting moves that improve model fit. Only matters if `num_mcmc_sweeps > 0`.


### Role of sigma...

The most important parameter from above is `sigma`. This controls the rate the model is collapsed. A slower (and lower) value will allow the model to explore more intermediate steps and make less drastic jumps between. 

We can see how `sigma` effects the speed of the collapse by looking at the total number of steps needed to fully collapse a network down to one remaining group by the total number of nodes in the network. 


```{r steps_needed_to_collapse, echo = FALSE}
expand_grid(
  num_nodes = seq(10, 250, by = 10),
  sigma = exp(seq(log(1.3), log(4), length.out = 6))
) %>% 
  mutate(
    steps_to_collapse = map2_dbl(
      num_nodes, 
      sigma, 
      function(n_blocks, sigma){
        n_steps <- 0
        while(n_blocks > 1){
          merges_to_make <- max(1, round(n_blocks*(1 - (1/sigma))))
          n_blocks <- n_blocks - merges_to_make
          n_steps <- n_steps + 1
        }
        n_steps
      }
    )
  ) %>% 
  ggplot(aes(x = num_nodes, y = steps_to_collapse)) +
  geom_line() +
  facet_wrap(~round(sigma,3)) +
  labs(title = "Number of steps needed to fully collapse groups for different sigma values",
       x = "Number of starting nodes",
       y = "Steps needed to collapse")
```

We can see that at `sigma > 2` the algorithm takes very few steps to collapse the network even as the network gets large. 

If we pin the network size at a constant (here 150) we can see the path of collapsing in terms of groups remaining for a variety of sigmas...

```{r merge_traces}
exp(seq(log(1.3), log(3), length.out = 6)) %>% 
  map_dfr(function(sigma, starting_num = 150, num_steps = 15){
    steps <- 1:(num_steps-1) %>% 
      reduce(function(group_nums, i){
        curr_num_blocks <- group_nums[i]
        merges_to_make <- max(1, round(curr_num_blocks*(1 - (1/sigma))))
        c(group_nums, max(curr_num_blocks - merges_to_make, 1))
      },
      .init = c(starting_num))
    
    tibble(
      size = steps, 
      sigma = round(sigma, 3), 
      step = 1:num_steps
    )
  }) %>% 
  ggplot(aes(x = step, y = size)) +
  geom_line() +
  facet_wrap(~sigma) +
  labs(title = "Number of clusters remaining by merge step for different sigma values",
       y = "Number of clusters remaining",
       x = "Step Number")
```

Again we see that the model drops very quickly at values of `sigma > 2`. 

We can see that with larger sigma values we get very rapid convergence to our desired model size. In theory this could cause issues like follows:

There are four groups in model (`{a,b,c,d}`). Groups `a` and `b` may be close to eachother and group `c` may be closer to `b` than it is do `d`. If we force two merges in a given step, `a`, and `b`, may be merged together and then, because those groups are merged and `c` is now left to merge with only `d`, which it does, forgoing the more optimal merging of `a->b->c`, `d`. In practive however, this scenario tends to not be an issue. 

If you want to merge groups together one at a time, specifying `sigma <= 1` will force that to happen. This can be combined with `num_mcmc_sweeps = 0` to give you a dendrogram-compatable collapse of your network. 


### Performing a single merge with 1 group per merge

To perform the complete merge we put our `sigma < 1` and our `num_mcmc_sweeps = 0`. In addition we return all the steps so we can look at the progress of the model as it collapses. 


```{r default_collapse_blocks}
collapse_results <- my_sbm %>% 
  collapse_blocks(sigma = 0.9, report_all_steps = TRUE)

head(collapse_results)
```

The results of the collapse function are a column with entropy of the model at a given state, and the number of groups in the model at that state. We can plot these using the included `visualize_collapse_results()` function to check for patterns. 

```{r plot_default_collapse}
visualize_collapse_results(collapse_results)
```

We see a nice smooth line going from one group per node down to a single group for all nodes. This line will always be monotonic increasing as the number of groups goes down. This is because removing more groups _always_ result in a poorer model fit because less information is being provided by the model structure. 



### Allowing for node shuffling

We can also let the model move the nodes around at each merge step which potentially allows it to get itself out of poor local minimums that the strict merging has gotten it into. We do this by setting `num_mcmc_sweeps > 0`.
 

```{r collapse_w_shuffle}
collapse_mcmc10 <- my_sbm %>% 
  collapse_blocks(sigma = 0.9, 
                  num_mcmc_sweeps = 10,
                  report_all_steps = TRUE)

visualize_collapse_results(collapse_mcmc10) +
  ggtitle("num_mcmc_sweeps = 10")
```

Now we see a completely different type of entropy path. The entropy decreases as we decrease the number of groups. This is a sign that the model is finding much better arrangements of its nodes into the groups through the MCMC steps. 

## Greedy Mode

By adding the greedy mode to this we in theory may find better merges because we explore all options. Let's see the impact. 

```{r collapse_w_greedy}
my_sbm %>% 
  collapse_blocks(sigma = 0.9, 
                  greedy = TRUE,
                  num_mcmc_sweeps = 10,
                  desired_num_blocks = 1,
                  report_all_steps = TRUE) %>% 
  visualize_collapse_results() +
  ggtitle("num_mcmc_sweeps = 10, greedy = TRUE")
```

Indeed we do see a bit of a smoothing out of the jaggedness of the model. 

## Raising sigma

One problem with these runs that collapse one group at a time is they are slow. As we saw, raising `sigma` can drastically reduce the number of steps needed. Let's try with `sigma = 2`. 

```{r collapse_w_higher_sigma}
collapse_sigma2 <- my_sbm %>% 
  collapse_blocks(sigma = 2, 
                  num_mcmc_sweeps = 10,
                  desired_num_blocks = 1,
                  report_all_steps = TRUE)

visualize_collapse_results(collapse_sigma2) +
  ggtitle("sigma = 2")
```

As expected, we get a lower-resolution line as fewer steps were taken. Let's overlay this faster collapse result with our `sigma < 1` result.


```{r compare_sigma_value_collapses}
bind_rows(
  collapse_mcmc10 %>% mutate(run = "sigma < 1"),
  collapse_sigma2 %>% mutate(run = "sigma = 2")
) %>% 
  ggplot(aes(x = num_blocks, y = entropy, color = run)) +
  geom_line()
```

So interestingly we see that while the runtime was way faster, our path had almost the same entropy values, which got closer as the model collapsed further. This shows that, if you're looking for the best collapse value increasing your sigma can save you lots of time without sacraficing much in the way of model performance. 

## Collapse Runs

So far we have looked at a single run of agglomerative collapsing, but the recomended algorithm in the paper the algorithm was introduced in, [Piexoto 2014](https://arxiv.org/abs/1310.4378), recomends running a seperate collapse for each number of groups, going from one up to a reasonable stopping point. 

Interestingly, due to the speed advantages of the high sigma values, even running seperate collapses for each target group number can be very fast compared to a single merge that looks at the possible group values. 

The function `collapse_run()` performs multiple merge runs to different numbers of groups and returns the results of the final merge collapsed together in the same form `collapse_blocks()` does with `report_all_steps = TRUE`. 

The only arguments it has that `collapse_blocks()` does not are:

- `num_final_blocks`: Array of group numbers that the agglomerative merge should target. E.g. 1:10.
- `parallel`: Should algorithm be run in parallel to speed up?

The parallel argument is particularly valuable as it exploits the fact that each run is independent of eachother and thus can be run on seperate threads for significant speedups. 

Lets try it on our model scanning from 1-10 groups. 

```{r parallel_collapse_run}
run_results <- my_sbm %>% 
  collapse_run(num_final_blocks = 1:10,
               parallel = TRUE)
```

```{r collapse_run_vs_collapse_blocks}
bind_rows(
  run_results %>% mutate(run = "collapse_run"),
  collapse_sigma2 %>% mutate(run = "collapse_blocks") %>% filter(num_blocks < 15)
) %>% 
  ggplot(aes(x = num_blocks, y = entropy, color = run)) +
  geom_line()
```

We see that for the overlaping portion we get better performance than the single group run and a high resolution as well. 

One other property of this run result is it seems smoother than the single `collapse_blocks()` result. This fact can be exploited to find the best partitioning. 

## Deciding the 'best' partition

Finding the "elbow" in graphs for choosing cluster results is an imperfect science. The package has a few built in heuristics to do so and also allows a completely custom function that takes either the entropy value of the runs or the entropy value and the number of groups. These heuristics can be seen using the `visualize_collapse_results()` function and used to select the best value using the `choose_best_collapse_state()` function. 

### Lowest

The simplest and most nieve method to find the best partition would be to simply take the lowest seen entropy. This is not a great idea because we know that entropy is monotonicly increasing as the number of groups decreases. The chances that the fit was so much better than others for the optimal number of groups is low. This is a very fast method to choose though and makes the least assumptions about the behavior of the system. 

```{r lowest_heuristic}
visualize_collapse_results(run_results, heuristic = 'lowest')
```



## Deviation from rolling mean

One method that works well for SBM models is looking at a given collapse results difference from the rolling mean of the entropy. Doing this serves to show what points show larger than expected dips in entropy for a single group being added. This is fit with `heuristic = 'dev_from_rolling_mean'`.


```{r dev_from_rolling_mean_heuristic}
visualize_collapse_results(run_results, heuristic = 'dev_from_rolling_mean')
```

The "best" point intuitively would be the point with the highest peak on the top chart. Here we see that peak occurs at __3__ groups. Which we know to be the true number of groups in this network (because we simulated it).

The nice thing about this metric is that it can be used in real time as we scan up the number of groups meaning the algorithm can stop at the value it needs by detecting a decline in the deviance function. 


## Non-Linear Least-Squares residuals

The final included heuristic takes into account the number of groups. It fits a non-linear regression model to the entropy as predicted by the number of groups. The equation it attempts to fit is $\text{entropy} \approx \beta_0 + \beta_1 \log(\text{num_blocks})$. Once this equation is fit each points residual from the fit is recorded. The point that is the farthest below the expected value according to the model is considered the 'best' fit. This is fit with `heuristic = 'nls_residual'`. 

```{r nls_heuristic}
visualize_collapse_results(run_results, heuristic = 'nls_residual')
```


## Loading the best fit

So far we have ignored the column `state` in our collapse results. This column is a list of dataframes, each containing the state of the model at a given collapse. This can be used to return the model to the state for the best group partitioning. 

This can be done with the `load_from_state()` function after retrieving the chosen state (e.g. `my_sbm %>% load_from_state(results$state[5])`) but a helper function is included to automate this process. `choose_best_collapse_state()` takes as input the dataframe returned by `collapse_run()` (or `collapse_block(report_all_steps = TRUE)`) and the heuristic used to find the best fit and selects the best result and loads it into the model. 

We can demonstrate with our collapse run results:

```{r selecting_best_partition}
best_partition <- my_sbm %>% 
  choose_best_collapse_state(run_results, 
                             heuristic = 'dev_from_rolling_mean', #default value
                             verbose = TRUE) # Tells us what state is chosen
```

Now our model is loaded with this collapse structure. Last we can visualize how this structure compares to the true groups. 

```{r comparing_partition_w_truth}
# Merge nodes partition results with true groups
nodes_w_inferred_block <- best_partition %>% 
  get_state() %>% 
  select(id, parent) %>% 
  right_join(network$nodes, by = 'id')

visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_block, 
                  node_color_col = 'block', 
                  node_shape_col = 'parent',
                  width = '100%')
```

We see a perfect match between the color and shape, meaning that every node was perfectly placed into its corresponding latent group. 

## Effects of other parameters on merge paths

While sigma is the most important parameter for collapse results tuning there are others we can tune. Let's look at how they effect the results. 

### Toggling Beta

We start with Beta. Beta is the inverse temperature parameter for MCMC move proposal acceptance. A higher value means that the model is more likely to select moves that fit the current structure of the SBM. This means it could potentially miss more optimal sollutions due to being stuck in a local minimum. However, it will also make the model converge faster as it wastes less time on poor moves. 

```{r toggling_beta}
# Select a range of beta values
beta_vals <- seq(1, 10, length.out = 8)

beta_sweep_results <- purrr::map_dfr(
  beta_vals, 
  function(beta){
    collapse_run(
      my_sbm,
      num_final_blocks = 1:8,
      beta = beta,
      parallel = TRUE
    ) %>% 
      mutate(beta = beta)
  }
)

beta_sweep_results %>% 
  ggplot(aes(x = num_blocks, y = entropy, color = beta)) +
  geom_line(aes(group = beta)) +
  geom_point() +
  scale_y_log10() +
  ggtitle("Collapse results by beta value")
```

So we can see that the behavior isnt changed much at all by the different betas. Most of the time the default value of `1.5` is fine. 

## Epsilon values

While beta controls how conservative the model is when choosing which moves to accept, epsilon controls how conservative the model is when generating those proposals. As epsilon goes to infinity the moves proposed by the model approach randomness. When epsilon is zero a node can only move to groups that nodes it is connected belong to.

While beta only came into plan when we had MCMC sweeps after each merge, epsilon comes into play at all times as the merge proposals are made by the same algorithm that does mcmc move proposals. 

```{r, toggling_epsilon}
# Get logaritmically spaced epsilon range
eps_vals <- exp(seq(log(0.1), log(15), length.out = 9))

eps_sweep_results <- purrr::map_dfr(
  eps_vals, 
  function(eps){
    my_sbm$EPS <- eps
    collapse_run(
      my_sbm,
      num_final_blocks = 1:8,
      parallel = TRUE
    ) %>% 
      mutate(eps = eps)
  }
)

eps_sweep_results %>% 
  ggplot(aes(x = num_blocks, y = entropy, color = eps)) +
  geom_line(aes(group = eps)) +
  geom_point() +
  scale_y_log10() +
  ggtitle("Collapse results by epsilon value")
```

While the results are all within error distance of eachother we do see there seems to be a trend of higher values getting better results, especially with the notable bump in the correct value at the correct 3 groups for one of the lower epsilon runs. 

## Looking at stability of collapse runs

By simply passing in a groups numbers to check vector with repeates in it we can acccess how stable a given collapse value is for a network. Here we will do five repeats for our network for the range of 2-8 groups. 

```{r run_stability}
num_repeats <- 5
group_nums <- 2:8

repeated_collapse <- collapse_run(
  my_sbm,
  num_final_blocks = rep(group_nums, each = num_repeats),
  parallel = TRUE
)

repeated_collapse %>% 
  ggplot(aes(x = num_blocks, y = entropy)) +
  geom_smooth(size = 0, method = 'loess', span = 0.75, fill = 'steelblue') +
  geom_jitter(alpha = 0.5, height = 0, width = 0.2)  +
  scale_x_continuous(breaks = group_nums)
```

We can see that the results are very stable with relatively minescule for values greater than 3 groups and no variation in the others (indicating they acheived the same partitioning every time). 


