---
title: "Sampling From Posterior"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sampling_from_posterior}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7
)
library(tidyverse)
library(glue)
library(sbmR)
```

```{r, eval = FALSE}
library(sbmR)
library(glue)
library(tidyverse)
```


In this document we will explore how to use the function `mcmc_sweep()` to sample from the posterior of a simulated network and investigate the results of doing so. 


## Setting up network structure

First we will generate our simulated network. First we set a seed to make the structure reproducable and then we draw our random network using the `sim_basic_block_network()` function to draw from a network with 3 groups and 40 nodes per group. 

```{r}
set.seed(312)

n_blocks <- 4    # Total number of blocks
block_size <- 75 # How many nodes in each block

network <- sim_basic_block_network(
  n_blocks = n_blocks,     
  n_nodes_per_block = block_size,  
  return_edge_propensities = TRUE
)
```


First we can make sure our edge propensities look reasonable 

```{r}
bind_rows(
  network$edge_propensities %>% rename(block_1 = block_2, block_2 = block_1),
  network$edge_propensities
) %>% 
  ggplot(aes(x = block_1, y = block_2, fill = propensity)) +
  geom_tile(color = 'white') +
  theme_bw() +
  labs(x = "", y = "")
```


Next, we can visualize the actual drawn network. 

```{r}
visualize_network(network, width = '100%')
```

We can see a decent amount of separation with perhaps more cohesion between the green and blue nodes than the orange


## Setting up SBM

Now that we have our data we can load it into an SBM object and look at the first few nodes worth of summary

```{r}
my_sbm <- create_sbm(network, eps = 0.01)

my_sbm %>% get_state() %>% head()
```


### Initializing the chain location

In an attempt to set our initial chain location to an optimal position we will agglomeratively merge the network using the collapse groups function. This will return a list of results for different model sizes. 

```{r}
collapse_results <- my_sbm %>% 
  collapse_run(num_block_proposals = 10, 
               sigma = 6,
               num_mcmc_sweeps = 10, 
               num_final_blocks = 1:15,
               parallel = TRUE)
```


```{r}
visualize_collapse_results(collapse_results, heuristic = 'dev_from_rolling_mean') +
  geom_vline(xintercept = n_blocks, color = 'orangered')
```


So we can use the helper function `choose_best_collapse_state()` to scan through the results and find the optimal partitioning based upon the designated hueristic. Right now it is just choosing the the maximum deviation from rolling mean (lower plot). Any scoring function can be provided as long as it takes as an argument the entropy column and returns a vector of scores where the highest value is the break to choose. 

```{r}
my_sbm <- choose_best_collapse_state(my_sbm, collapse_results, heuristic = "dev_from_rolling_mean", verbose = TRUE)
```

### Visualizing these results. 

We can investigate the structure of this chosen partitioning against the true structure that we simulated using `visualize_network()`...

```{r extract_infered_blocks}
merged_state <- my_sbm %>% 
  get_state() %>% 
  select(id, parent)

nodes_w_inferred_block <- network$nodes  %>%
  left_join(merged_state, by = 'id') %>% 
  rename(inferred = parent)

table(nodes_w_inferred_block$inferred,nodes_w_inferred_block$block)
```

```{r visualize_inferred_blocks}
visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_block, 
                  node_color_col = 'block', 
                  node_shape_col = 'inferred',
                  width = '100%')
```


This should be a decent starting place for our sampling.



## Sampling from the posterior

Now that we have our model in a decent starting place we can initiate the MCMC sampling. We will start by doing a few sweeps to see if our model shape has stabalized at all. 

```{r}
num_sweeps <- 200

sweep_results <- mcmc_sweep(my_sbm, num_sweeps = num_sweeps, track_pairs = FALSE)

sweep_df <- sweep_results$sweep_info %>% 
  mutate(sweep = 1:n(),
         label = 'eps = 0.1') %>% 
  pivot_longer(entropy_delta:num_nodes_moved)

plot_sweep_results <- function(sweep_results_df){
   sweep_results_df %>% 
    ggplot(aes(x = sweep, y = value)) +
    geom_line() +
    facet_grid(name~., scales = "free_y")
}

plot_sweep_results(sweep_df) +
  labs(
    title = glue::glue('Result of {num_sweeps} MCMC sweeps'),
    subtitle = "Entropy Delta of sweep and number of nodes moved for sweep"
  )
```



```{r}
nodes_w_inferred_block <- my_sbm %>% 
  get_state() %>% 
  select(id, parent) %>% 
  right_join(network$nodes, by = 'id') %>% 
  rename(inferred = parent)

table(nodes_w_inferred_block$inferred,nodes_w_inferred_block$block)
```

```{r}
visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_block, 
                  node_color_col = 'block', 
                  node_shape_col = 'inferred',
                  width = '100%')
```

So even though we have more groups than we should, we never have an infered group that spans more than a single true group. 


We can tweak the results MCMC sweeps by adjusting the `EPS` or 'epsilon' parameter.

This controls how random the moves proposed are. The higher epsilon the more random the moves will be. It's like adding a constant to all the nodes probability of being chosen as a proposed move. As the constant gets higher it starts to overpower the data. This value can be set when initializing the network with `create_sbm(eps = ...)` or modified post creation by targeting the sbm class value of `EPS`.

Let's first repeat another `r num_sweeps` sweeps with a lower epsilon. We will recreate our model from the data and then reinitialize it to the collapsed state choice. 


```{r}
# Re-initialize network with new epsilon (old epsilon was 0.1) and reload the collapsed state
lower_eps <- 0.005
my_sbm <- create_sbm(network, eps = lower_eps) %>% 
  choose_best_collapse_state(collapse_results, heuristic = 'lowest')


# Run our mcmc sweeps again
sweep_df_lower_eps <- mcmc_sweep(my_sbm, num_sweeps = 200)$sweep_info %>% 
  mutate(sweep = 1:n(),
         label = glue::glue('eps = {lower_eps}')) %>% 
  pivot_longer(entropy_delta:num_nodes_moved)


plot_sweep_results(sweep_df_lower_eps) +
  labs(title = glue::glue('Result of {num_sweeps} MCMC sweeps w/ eps = {lower_eps}'))
```

### Comparing samples

Now we can put the two sweep runs side-by-side to see how they compare. We see that on average the results of the lower epsilon value are more stable, which intuitively makes sense as the model won't try too many crazy proposals. 

```{r}
bind_rows(
  sweep_df,
  sweep_df_lower_eps
) %>% 
  plot_sweep_results() +
  facet_grid(name~label, scales = "free_y")
```

## Pairwise block consensus

The `mcmc_sweep()` method has an option to keep track of how connected each node is with all other nodes. The idea being that the more frequently that two nodes are in the same block together, the more confident the model is that they are truly connected. We can visualize this by accessing the `pairing_counts` dataframe attached to the outputs of `mcmc_sweep()` with `track_pairs = TRUE` (as we did in the first sweeps of `eps = 0.1`).


```{r}
sweep_results$pairing_counts %>% 
  ggplot(aes(x = node_a, y = node_b, fill = proportion_connected)) +
  geom_raster() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(x = "node a", 
       y = "node b",
       fill = "Proportion of sweeps\nin same block",
       title = "Node block consensus matrix",
       subtitle = glue::glue("True number of blocks: {n_blocks}"))
```

