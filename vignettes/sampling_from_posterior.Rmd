---
title: "Sampling From Posterior"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sampling_from_posterior}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7
)
```

```{r setup}
library(sbmR)
library(tidyverse)
```


In this document we will explore how to use the function `mcmc_sweep()` to sample from the posterior of a simulated network and investigate the results of doing so. 


## Setting up network structure

First we will generate our simulated network. First we set a seed to make the structure reproducable and then we draw our random network using the `sim_basic_block_network()` function to draw from a network with 3 groups and 40 nodes per group. 

```{r}
set.seed(42)

group_info <- dplyr::tribble(
  ~group, ~n_nodes,
     "a",       20,
     "b",       22,
     "c",       25
)

connection_propensities <- dplyr::tribble(
 ~group_1, ~group_2, ~propensity,
      "a",      "a",         0.8,
      "a",      "b",         0.2,
      "a",      "c",         0.3,
      "b",      "b",         0.9,
      "b",      "c",         0.15,
      "c",      "c",         0.4,
)

network <- sim_sbm_network(group_info, connection_propensities, edge_dist = purrr::rbernoulli)
```


First we can make sure our connection propensities look reasonable 

```{r}
connection_propensities %>% 
  ggplot(aes(x = group_1, y = group_2, fill = propensity)) +
  geom_tile(color = 'white') +
  theme_bw()
```


Next, we can visualize the actual drawn network. 

```{r}
visualize_network(network, width = '100%')
```

We can see a decent amount of separation with perhaps more cohesion between the green and blue nodes than the orange


## Setting up SBM

Now that we have our data we can load it into an SBM object and look at the first few nodes worth of summary

```{r}
my_sbm <- create_sbm(network)

my_sbm %>% get_state() %>% head()
```


### Initializing the chain location

In an attempt to set our initial chain location to an optimal position we will agglomeratively merge the network using the collapse groups function. This will return a list of results for different model sizes. 

```{r}
collapse_results <- my_sbm %>% 
  collapse_groups(exhaustive = TRUE, num_group_proposals = 8, num_mcmc_sweeps = 25)
```


Let's look at the collapse results in terms of number of groups and the size of the network...

```{r}
ggplot(collapse_results, aes(x = num_groups, y = entropy)) +
  geom_line() +
  scale_x_log10() +
  geom_vline(xintercept = 3, color = 'orangered')
```

So even though we know our true number of groups is `3` we see a minimum entropy at `r collapse_results$num_groups[collapse_results$entropy == min(collapse_results$entropy)]` so we will start our chain there. To do this we will use the helper function `choose_best_collapse_state()` which will scan through the results and find the optimal partitioning based upon the designated hueristic. Right now it is just choosing the lowest value. (In the future more complicated methods will be allowed.)

```{r}
my_sbm <- my_sbm %>% choose_best_collapse_state(collapse_results, heuristic = 'lowest')
```


### Visualizing these results. We can use our visualization function to look at the results of this clustering and compare that to the known structure...

```{r}
merged_state <- my_sbm %>% 
  get_state() %>% 
  select(id, parent)

nodes_w_inferred_group <- network$nodes  %>%
  left_join(merged_state, by = 'id') %>% 
  rename(inferred = parent)

visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_group, 
                  node_color_col = 'group', 
                  node_shape_col = 'inferred',
                  width = '100%')
```

It appears that our grouping is working decently well as the cluster of nodes that represent the orange group seem to be made up of the same types of shapes whereas the other groups are more mixed as we would expect. 

## Sampling from the posterior

Now that we have our model in a decent starting place we can initiate the MCMC sampling. We will start by doing 100 sweeps to see if our model shape has stabalized at all. 

```{r}
num_sweeps <- 1000

sweep_results <- 1:num_sweeps %>% 
  purrr::map(~mcmc_sweep(my_sbm))


plot_sweep_results <- function(sweep_results){
  sweep_results %>% 
    map_dfr(~tibble(
      n_moved = length(.$nodes_moved),
      entropy_delta = .$entropy_delta
    )) %>% 
    mutate(sweep = 1:n()) %>% 
    pivot_longer(-sweep, names_to = 'stat') %>% 
    ggplot(aes(x = sweep, y = value)) +
    geom_line() +
    facet_grid(stat~., scales = 'free_y')
}

plot_sweep_results(sweep_results) +
  labs(
    title = glue::glue('Result of {num_sweeps} MCMC sweeps'),
    subtitle = "Entropy Delta of sweep and number of nodes moved for sweep"
  )
```



```{r}
nodes_w_inferred_group <- my_sbm %>% 
  get_state() %>% 
  select(id, parent) %>% 
  right_join(network$nodes, by = 'id') %>% 
  rename(inferred = parent)

visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_group, 
                  node_color_col = 'group', 
                  node_shape_col = 'inferred',
                  width = '100%')
```

We can tweak the results MCMC sweeps by adjusting two parameters. 

1. `EPS` or 'epsilon' - This controls how random the moves proposed are. The higher epsilon the more random the moves will be. It's like adding a constant to all the nodes probability of being chosen as a proposed move. As the constant gets higher it starts to overpower the data. This value can be set when initializing the network with `create_sbm(eps = ...)` or modified post creation by targeting the sbm class value of `EPS`.
2. `beta` - This is an 'inverse temperature parameter' that effects the probability of accepting a move. The lower this value the less important the move's impact on model entropy is to its being chosen. This value is passed to the `mcmc_sweep()` function.


Let's first repeat another `r num_sweeps` sweeps with a lower epsilon. We will recreate our model from the data and then reinitialize it to the collapsed state choice. 


```{r}
# Re-initialize network with new epsilon (old epsilon was 0.1) and reload the collapsed state
my_sbm <- create_sbm(network, eps = 0.01) %>% 
  choose_best_collapse_state(collapse_results, heuristic = 'lowest')

# Run our mcmc sweeps again
sweep_results_lower_eps <- 1:num_sweeps %>% 
  purrr::map(~mcmc_sweep(my_sbm))

plot_sweep_results(sweep_results_lower_eps) +
  labs(title = glue::glue('Result of {num_sweeps} MCMC sweeps w/ eps = 0.01'))
```

```{r}
run_mcmc_sweeps <- function(n_sweeps, just_init = FALSE){
  # Re-initialize network with new epsilon (old epsilon was 0.1) and reload the collapsed state
  my_sbm <- create_sbm(network, eps = 0.01) %>% 
    choose_best_collapse_state(collapse_results, heuristic = 'lowest')
  
  if(just_init) return(NULL)
  # Run sweeps
  purrr::map(1:num_sweeps, ~mcmc_sweep(my_sbm))
}


n_sweeps <- 1000
full_runs <- bench::mark(run_mcmc_sweeps(n_sweeps))
just_inits <- bench::mark(run_mcmc_sweeps(n_sweeps, just_init = TRUE))

median_time <- full_runs$median - just_inits$median
```

