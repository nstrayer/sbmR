---
title: "Sampling From Posterior"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sampling_from_posterior}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7
)
library(tidyverse)
library(glue)
library(sbmR)
```

```{r, eval = FALSE}
library(sbmR)
library(glue)
library(tidyverse)
```


In this document we will explore how to use the function `mcmc_sweep()` to sample from the posterior of a simulated network and investigate the results of doing so. 


## Setting up network structure

First we will generate our simulated network. First we set a seed to make the structure reproducable and then we draw our random network using the `sim_basic_block_network()` function to draw from a network with 3 groups and 40 nodes per group. 

```{r}
set.seed(42)

n_blocks <- 4    # Four total groups
group_size <- 50 # W/ 50 nodes in each

network <- sim_basic_block_network(
  n_blocks = n_blocks,     
  n_nodes_per_block = group_size,  
  return_edge_propensities = TRUE
)
```


First we can make sure our edge propensities look reasonable 

```{r}
network$edge_propensities %>% 
  ggplot(aes(x = group_1, y = group_2, fill = propensity)) +
  geom_tile(color = 'white') +
  theme_bw()
```


Next, we can visualize the actual drawn network. 

```{r}
visualize_network(network, width = '100%')
```

We can see a decent amount of separation with perhaps more cohesion between the green and blue nodes than the orange


## Setting up SBM

Now that we have our data we can load it into an SBM object and look at the first few nodes worth of summary

```{r}
my_sbm <- create_sbm(network)

my_sbm %>% get_state() %>% head()
```


### Initializing the chain location

In an attempt to set our initial chain location to an optimal position we will agglomeratively merge the network using the collapse groups function. This will return a list of results for different model sizes. 

```{r}
collapse_results <- my_sbm %>% 
  collapse_run(num_block_proposals = 15, 
               sigma = 6,
               num_mcmc_sweeps = 15, 
               num_final_blocks = 1:15,
               parallel = TRUE)
```


```{r}
visualize_collapse_results(collapse_results, heuristic = 'dev_from_rolling_mean') +
  geom_vline(xintercept = n_blocks, color = 'orangered')
```


So we can use the helper function `choose_best_collapse_state()` to scan through the results and find the optimal partitioning based upon the designated hueristic. Right now it is just choosing the the maximum deviation from rolling mean (lower plot). Any scoring function can be provided as long as it takes as an argument the entropy column and returns a vector of scores where the highest value is the break to choose. 

```{r}
my_sbm <- choose_best_collapse_state(my_sbm, collapse_results, heuristic = "dev_from_rolling_mean", verbose = TRUE)
```

### Visualizing these results. 

We can investigate the structure of this chosen partitioning against the true structure that we simulated using `visualize_network()`...

```{r}
merged_state <- my_sbm %>% 
  get_state() %>% 
  select(id, parent)

nodes_w_inferred_block <- network$nodes  %>%
  left_join(merged_state, by = 'id') %>% 
  rename(inferred = parent)

visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_block, 
                  node_color_col = 'group', 
                  node_shape_col = 'inferred',
                  width = '100%')
```

The colors and shapes seem to match very well. We can check the aggreement as well.

```{r}
nodes_w_inferred_block %>% 
  group_by(group) %>% 
  summarise(n_unique_inferred = length(unique(inferred)))
```

So each group was mapped into by only one class, meaning we perfectly matched the structure. 



## Sampling from the posterior

Now that we have our model in a decent starting place we can initiate the MCMC sampling. We will start by doing 500 sweeps to see if our model shape has stabalized at all. 

```{r}
num_sweeps <- 500

sweep_results_to_df <- function(sweep_results, label = 'results'){
  sweep_results %>% 
    map_dfr(~tibble(
      n_moved = length(.$nodes_moved),
      entropy_delta = .$entropy_delta
    )) %>% 
    mutate(sweep = 1:n()) %>% 
    pivot_longer(-sweep, names_to = 'stat') %>% 
    mutate(label = label)
}

sweep_results <- 1:num_sweeps %>% 
  purrr::map(~mcmc_sweep(my_sbm)) %>% 
  sweep_results_to_df(label = 'eps = 0.1')


plot_sweep_results <- function(sweep_results_df){
   sweep_results_df %>% 
    ggplot(aes(x = sweep, y = value)) +
    geom_line() +
    facet_grid(stat~., scales = 'free_y')
}

sweep_results %>% 
  plot_sweep_results() +
  labs(
    title = glue::glue('Result of {num_sweeps} MCMC sweeps'),
    subtitle = "Entropy Delta of sweep and number of nodes moved for sweep"
  )
```



```{r}
nodes_w_inferred_block <- my_sbm %>% 
  get_state() %>% 
  select(id, parent) %>% 
  right_join(network$nodes, by = 'id') %>% 
  rename(inferred = parent)

visualize_network(edges = network$edges, 
                  nodes = nodes_w_inferred_block, 
                  node_color_col = 'group', 
                  node_shape_col = 'inferred',
                  width = '100%')
```

```{r}
nodes_w_inferred_block %>% 
  group_by(inferred) %>% 
  summarise(n_unique_blocks = length(unique(group)))
```

So even though we have more groups than we should, we never have an infered group that spans more than a single true group. 


We can tweak the results MCMC sweeps by adjusting two parameters. 

1. `EPS` or 'epsilon' - This controls how random the moves proposed are. The higher epsilon the more random the moves will be. It's like adding a constant to all the nodes probability of being chosen as a proposed move. As the constant gets higher it starts to overpower the data. This value can be set when initializing the network with `create_sbm(eps = ...)` or modified post creation by targeting the sbm class value of `EPS`.
2. `beta` - This is an 'inverse temperature parameter' that effects the probability of accepting a move. The lower this value the less important the move's impact on model entropy is to its being chosen. This value is passed to the `mcmc_sweep()` function.


Let's first repeat another `r num_sweeps` sweeps with a lower epsilon. We will recreate our model from the data and then reinitialize it to the collapsed state choice. 


```{r}
# Re-initialize network with new epsilon (old epsilon was 0.1) and reload the collapsed state
lower_eps <- 0.005
my_sbm <- create_sbm(network, eps = lower_eps) %>% 
  choose_best_collapse_state(collapse_results, heuristic = 'lowest')

# Run our mcmc sweeps again
sweep_results_lower_eps <- 1:num_sweeps %>% 
  purrr::map(~mcmc_sweep(my_sbm)) %>% 
  sweep_results_to_df(label = glue('eps = {lower_eps}'))

sweep_results_lower_eps %>% 
  plot_sweep_results() +
  labs(title = glue::glue('Result of {num_sweeps} MCMC sweeps w/ eps = {lower_eps}'))
```

### Comparing samples

Now we can put the two sweep runs side-by-side to see how they compare. We see that on average the results of the lower epsilon value are more stable, which intuitively makes sense as the model won't try too many crazy proposals. 

```{r}
bind_rows(
  sweep_results,
  sweep_results_lower_eps
) %>% 
  plot_sweep_results() +
  facet_grid(stat~label, scales = 'free_y')
```

