---
title: "Agglomerative Merging"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{agglomerative_merging}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(sbmR)
library(tidyverse)
```

One technique of finding the optimal number of clusters/ best partitioning is using the efficient agglomerative merging algorithm. This is typically used to find the initial state for MCMC chains but can stand on its own as a clustering method. 

Agglomerative merging can be acomplished with the SBM class using the method `collapse_groups()`. 


First we will setup a model using a simulated network dataset built using the included `sim_basic_block_network()` function.


```{r, eval = FALSE}
library(sbmR)
library(tidyverse)
```


```{r}
set.seed(42)

n_groups <- 4    # Four total groups
group_size <- 50 # W/ 50 nodes in each

network <- sim_basic_block_network(
  n_groups = n_groups,     
  n_nodes_per_group = group_size,  
  return_connection_propensities = TRUE
)
```

We can investigate the simulated network by visualizing the connection propensities...
```{r}
network$connection_propensities %>% 
  ggplot(aes(x = group_1, y = group_2)) +
  geom_tile(aes(fill = propensity))
```

and we can visualize the network structure using the `visualize_network()` function.
```{r}
visualize_network(network)
```


Now that we have our network we can initialize an SBM model with it. 

```{r}
# Setup SBM model
my_sbm <- create_sbm(network)
my_sbm %>% get_state() %>% head()
```

## Basic merging

Now that we have our model setup there are two parameters that control this merging. 

First we have `GREEDY`. When this is set to `TRUE`, the at every merge step the model will check every possible merge for each node and select the best one. 

Second is `N_CHECKS_PER_GROUP`. When `GREEDY` is set to `FALSE`, this parameter controls how many possible merges the model explores for each node before choosing the best one. Higher means a longer run time. Note that these potential merges are done by a random sample of neighbor groups so even if `N_CHECKS_PER_GROUP` is set to the total number of nodes in the network we are not guarenteed to explore all options like `GREEDY` does. 

First, we will perform a greedy merging. This is done with the `collapse_groups()` method. 

```{r}

library(furrr)
plan(multiprocess)

collapse_to_desired_num <- function(desired_num){
  create_sbm(network) %>% 
  collapse_groups(desired_num_groups = desired_num, 
                  sigma = 2,
                  num_group_proposals = 5,
                  num_mcmc_sweeps = 5) %>% 
    select(-state) %>% 
    mutate(desired_num = desired_num)
}

r_sequential <- function(){
  purrr::map_dfr(
    1:10,
    collapse_to_desired_num
  )
}

r_parallel <- function(){
  merge_sweep <- future_map_dfr(
    1:10,
    collapse_to_desired_num
  )
}

cpp_sequential <- function(){
  create_sbm(network) %>% 
    collapse_run()
}

plan(multiprocess)


tic()
merge_sweep <- purrr::map_dfr(
  1:10,
  collapse_to_desired_num
)
toc()

tic()
merge_sweep <- future_map_dfr(
  1:10,
  collapse_to_desired_num
)
toc()


merge_sweep %>% 
  ggplot(aes(x = desired_num, y = entropy)) + 
  geom_point() +
  geom_segment(aes(xend = num_groups, yend = entropy)) +
  geom_line() + 
  geom_vline(xintercept = n_groups)
```


```{r}
# Run initialization algorithm on base level nodes
merge_results <- collapse_groups(my_sbm, greedy = TRUE)
```

This method returns a dataframe containing the entropy, number of groups, and state of the model at each merge step. We can use these to look at how the merges progressed. 


```{r}
merge_results %>% select(-state) %>% head()
```


We should almost always see the entropy decrease as we remove groups but if there is signal in the model and the algorithm found it, we should see a relativel "lack of decrease" around the true structure value. We know the true number of groups in this data is 6 so we can reference that here...

```{r}
plot_merge_results <- function(merge_results, group_cutoff = 30, trim_axes = TRUE){
  
  results_below_cutoff <- merge_results %>% 
    filter(num_groups < group_cutoff)
  
  max_entropy <- max(results_below_cutoff$entropy)
  min_entropy <- min(results_below_cutoff$entropy)
  
  p <- ggplot(merge_results, aes(x = num_groups, y = entropy)) +
    geom_line(alpha = 0.3) +
    geom_point(size = 0.5) +
    geom_vline(xintercept = n_groups, color = 'orangered')
    
  if(trim_axes){
    p <- p +
      coord_cartesian(
        xlim = c(0, group_cutoff),
        ylim = c(min_entropy, max_entropy)
      ) 
  }
  
  p +
    scale_x_reverse() +
    theme_minimal() +
    labs(x = "number of groups")
}
```


```{r}
merge_results %>% plot_merge_results()
```

## Non-Greedy merge proposals


We can perform the same setup but this time not using greedy but the sampling version
```{r}
my_sbm %>% 
  collapse_groups(greedy = FALSE, num_group_proposals = 10) %>% 
  plot_merge_results()
```


## Controlling number of merge checks per node

We can try scanning over different number of checks per group to see how it changes results

```{r}
merges_over_changes <- purrr::map_dfr(
  seq(5, 50, by = 5),
  ~{
    run_time <- system.time({
      merge_results <- collapse_groups(my_sbm, num_group_proposals = .x)
    })
    
    merge_results %>% 
      mutate(checks = .x, run_time = run_time[1])
  })

plot_merge_results(merges_over_changes) + 
  facet_wrap(~checks) +
  ggtitle("Tuning number of merge checks per node")
```

```{r}
greedy_run_time <- system.time({
  collapse_groups(create_sbm(edges, nodes), greedy = TRUE) 
}) %>% pluck(1)

merges_over_changes %>% 
  group_by(checks) %>% 
  summarise(run_time = first(run_time)) %>% 
  ggplot(aes(x = checks, y = run_time)) +
  geom_line() + 
  geom_hline(yintercept = greedy_run_time, color = 'orangered') +
  labs(title = "Run time by number of merges checked per group",
       subtitle = "Horizontal line is runtime of greedy merging")
```

## Controlling number of MCMC sweeps between merge steps

Another tuning parameter that will effect results is how many mcmc sweeps are performed between each merge step. When we let the model perform more MCMC sweeps in theory it will equilibriate to the best partition with the current number of groups, meaning we would get a better picture of the best entropy for a given number of clusters. 

Here we scan over different numbers of MCMC sweeps and if we use greedy or not for our merging (in theory greedy should not be particularly helpful here.)

```{r}
merges_over_sweeps_and_greedy <- expand.grid(
  mcmc_sweeps = seq(1, 101, by = 33),
  greedy = c(TRUE, FALSE)
) %>% 
  {
    purrr::map2_dfr(
      .$mcmc_sweeps,
      .$greedy,
      ~{
        create_sbm(edges, nodes) %>% 
          collapse_groups(greedy = .y, num_mcmc_sweeps = .x) %>% 
          mutate(mcmc_sweeps = .x, greedy = paste("greedy = ", .y))
      }
    )
  }


plot_merge_results(merges_over_sweeps_and_greedy) + 
  facet_grid(mcmc_sweeps~greedy) +
  ggtitle("Tuning and greedy status and number of mcmc sweeps per stage")
```


## Controlling collapsing rate (sigma)

Another important parameter to tune is `SIGMA`. This controls the rate that the network collapses. For each merge step the model will remove `B_curr/SIGMA` groups. This means a value of `SIGMA = 2` will cut the group size in half each step. If we were to set `SIGMA` below `1`, this would force the model to remove a single group at a time (if no MCMC sweeps are happening) because `B_curr/{x<1} > B_curr` so the smallest possible merge of a single group is taken.


```{r}
purrr::map_dfr(
  seq(from = 0.9, to = 2, length.out = 5),
  ~{
    create_sbm(edges, nodes) %>% 
      collapse_groups(exhaustive = FALSE, sigma = .x, num_mcmc_sweeps = 0) %>% 
      mutate(sigma = .x)
  }) %>% 
  plot_merge_results(trim_axes = FALSE) + 
  facet_wrap(~sigma) +
  ggtitle("Tuning sigma value for merging")
```

